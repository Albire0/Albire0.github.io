<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>First - 我的全新 Hugo 网站</title><meta name="Description" content="这是我的全新 Hugo 网站"><meta property="og:title" content="First" />
<meta property="og:description" content="NLP课程详细设计-报告 环境准备 Windows10 Pycharm2021 python3.9 剩余的依赖由文件夹下的requirements.txt提供,为以下这些 1 2 3 4 5 6 7 8 9 jieba==0.42.1 keras==2.8.0 Keras_Preprocessing==1.1.2 matplotlib==3.5.2 numpy==1.21.4 pandas==1.3.5 scikit_learn==1.1.1" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://albire0.github.io/post/first/" /><meta property="og:image" content="https://albire0.github.io/logo.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-11-13T21:14:22+08:00" />
<meta property="article:modified_time" content="2022-11-13T23:32:16+08:00" /><meta property="og:site_name" content="我的网站" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://albire0.github.io/logo.png"/>

<meta name="twitter:title" content="First"/>
<meta name="twitter:description" content="NLP课程详细设计-报告 环境准备 Windows10 Pycharm2021 python3.9 剩余的依赖由文件夹下的requirements.txt提供,为以下这些 1 2 3 4 5 6 7 8 9 jieba==0.42.1 keras==2.8.0 Keras_Preprocessing==1.1.2 matplotlib==3.5.2 numpy==1.21.4 pandas==1.3.5 scikit_learn==1.1.1"/>
<meta name="application-name" content="我的网站">
<meta name="apple-mobile-web-app-title" content="我的网站"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://albire0.github.io/post/first/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="/lib/fontawesome-free/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"></noscript><link rel="preload" href="/lib/animate/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="/lib/animate/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "First",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/albire0.github.io\/post\/first\/"
        },"genre": "post","wordcount":  3208 ,
        "url": "https:\/\/albire0.github.io\/post\/first\/","datePublished": "2022-11-13T21:14:22+08:00","dateModified": "2022-11-13T23:32:16+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "xxxx"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="我的全新 Hugo 网站"></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/post/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="我的全新 Hugo 网站"></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/post/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="page single special"><h1 class="single-title animate__animated animate__pulse animate__faster">First</h1><div class="content" id="content"><h1 id="nlp课程详细设计-报告">NLP课程详细设计-报告</h1>
<h2 id="环境准备">环境准备</h2>
<p>Windows10</p>
<p>Pycharm2021</p>
<p>python3.9</p>
<p>剩余的依赖由文件夹下的requirements.txt提供,为以下这些</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">jieba==0.42.1
</span></span><span class="line"><span class="cl">keras==2.8.0
</span></span><span class="line"><span class="cl">Keras_Preprocessing==1.1.2
</span></span><span class="line"><span class="cl">matplotlib==3.5.2
</span></span><span class="line"><span class="cl">numpy==1.21.4
</span></span><span class="line"><span class="cl">pandas==1.3.5
</span></span><span class="line"><span class="cl">scikit_learn==1.1.1
</span></span><span class="line"><span class="cl">tqdm==4.62.3
</span></span><span class="line"><span class="cl">gensim==4.1.2
</span></span></code></pre></td></tr></table>
</div>
</div><p>数据集选用美团的ASAP-SENT数据集</p>
<h2 id="设计流程">设计流程</h2>
<p>对于一个字，词的情感分析模型，设计实验比较字的效果是否好于词，流程为</p>
<ol>
<li>数据处理
<ol>
<li>读取文件中的文本和对应的标签，最大文本的长度</li>
<li>观察标签的分布情况，发现3，2，1的数量要明显小于4，5</li>
<li>将3，2，1集合为1类，4，5分别为第2，第3类</li>
<li>按字/词的方式，构造word2id字典</li>
<li>将文本中的字/词按word2id字典做映射，截断到最大文本长</li>
</ol>
</li>
<li>数据处理（词向量）
<ol>
<li>分词后训练Word2Vec模型</li>
<li>创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引</li>
<li>准备词向量矩阵</li>
</ol>
</li>
<li>搭建模型
<ol>
<li>按照下面的模型结构搭建</li>
<li>模型的优化器选用adam，学习率默认</li>
<li>模型的评估方法为loss和acc</li>
<li>模型的损失函数为稀疏交叉熵</li>
</ol>
</li>
<li>训练模型
<ol>
<li>训练集和验证集的比例为9：1</li>
<li>直接将x，y整体输入模型中，不使用生成器</li>
</ol>
</li>
<li>对每个模型绘制loss和acc的图</li>
<li>观察结果，分析</li>
</ol>
<h2 id="数据处理">数据处理</h2>
<p>数据的处理有两种情况，字的部分通过data.py文件中的函数，词的处理通过MyTokenizer类</p>
<p>在data.py中，描述如下</p>
<table>
<thead>
<tr>
<th>作用</th>
<th>函数名</th>
</tr>
</thead>
<tbody>
<tr>
<td>将句子转化为token</td>
<td>def sentence_to_token(sentence, word2token: dict):-&gt;tokens</td>
</tr>
<tr>
<td>对每一条句子做补0或截断操作</td>
<td>def padding_or_truncate_text(item):-&gt;texts</td>
</tr>
<tr>
<td>读取训练数据</td>
<td>def read_train_data()</td>
</tr>
<tr>
<td>获得各项分类的权重</td>
<td>def get_classweights(y:list[int]):</td>
</tr>
<tr>
<td>获取word2id的字典</td>
<td>def get_word2id_json(sen_list):</td>
</tr>
<tr>
<td>将所有的星级转化为标签分类</td>
<td>def get_labels(stars):</td>
</tr>
<tr>
<td>将好评的星级转化为3个分类中的一个</td>
<td>def to_label(star):</td>
</tr>
<tr>
<td>统计各个star的出现频率</td>
<td>def calculate_star(stars):</td>
</tr>
<tr>
<td>将所有句子转化为tokens</td>
<td>def get_result(text_a, word2token):</td>
</tr>
</tbody>
</table>
<p>MyTokenizer类中，描述如下</p>
<p>初始化函数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">char_level</span><span class="p">,</span> <span class="n">max_word</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="s2">&#34;&lt;unk&gt;&#34;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                 <span class="n">filters</span><span class="o">=</span><span class="s1">&#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[</span><span class="se">\\</span><span class="s1">]^_`{|}~</span><span class="se">\t\n</span><span class="s1">&#39;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        Tokenizer的初始化,返回单例
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param char_level:False按词去分,True 按字去分
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param max_word:最多分词的个数,默认是为-1,是所有
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param oov_token:out of vocab的单词对应的token
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param lower: 是否全部转化为小写 默认为True
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param filters: filters字符串内,是所有不参与分词的字符 默认为 &#39;!&#34;#$%&amp;()*+,-./:;&lt;=&gt;?@[</span><span class="se">\\</span><span class="s2">]^_`{|}~</span><span class="se">\t\n</span><span class="s2">&#39;
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>单例构造</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        借助new方法 实现单例模式,这是因为new执行后,才是init执行
</span></span></span><span class="line"><span class="cl"><span class="s2">        @:return 返回MyTokenizer单例
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>获取Mytokenizer的构造参数</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        获取Tokenizer的组装属性,并打印
</span></span></span><span class="line"><span class="cl"><span class="s2">        :return:返回Tokenizer的组装属性,type:dict
</span></span></span></code></pre></td></tr></table>
</div>
</div><p>把所有的句子变成word_level的</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">text_to_wordlevel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="n">texts</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        把普通的中文文本,转换为以词为单位,并保存
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param texts: :param texts: 文章 shape为(list[sentence])
</span></span></span><span class="line"><span class="cl"><span class="s2">        比如:[&#34;飘渺孤鸿影，漏断人初静&#34;,”想当年，金戈铁马，气吞万里如虎“]
</span></span></span><span class="line"><span class="cl"><span class="s2">        :return:以词为单位的文本新数组
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s2">&#34;word_level_input.txt&#34;</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;word_level_input.txt&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">buffers</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">readlines</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">            <span class="n">new_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">item</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">buffers</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;word_level_input.txt&#34;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">new_texts</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">texts</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">                <span class="n">transformed_text</span> <span class="o">=</span> <span class="s2">&#34; &#34;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">jieba</span><span class="o">.</span><span class="n">lcut</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="n">index</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">                <span class="n">f</span><span class="o">.</span><span class="n">writelines</span><span class="p">(</span><span class="n">transformed_text</span><span class="o">+</span><span class="s2">&#34;</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">new_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">transformed_text</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">new_texts</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>利用text初始化tokenizer，得到字典</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">fit_on_texts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]):</span>
</span></span><span class="line"><span class="cl">        <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">        tokenizer获取对应的word2id,和id2word
</span></span></span><span class="line"><span class="cl"><span class="s2">        :param texts: 文章 shape为(list[sentence])
</span></span></span><span class="line"><span class="cl"><span class="s2">        比如:[&#34;飘渺孤鸿影，漏断人初静&#34;,”想当年，金戈铁马，气吞万里如虎“]
</span></span></span><span class="line"><span class="cl"><span class="s2">        :return:word2id:dict,id2word:dict
</span></span></span><span class="line"><span class="cl"><span class="s2">        &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&#34;texts must be a list[str] type&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">__char_level</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">new_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_to_wordlevel</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">__TOKENIZER</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">new_texts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="bp">self</span><span class="o">.</span><span class="n">__TOKENIZER</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">word2id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__TOKENIZER</span><span class="o">.</span><span class="n">word_index</span>
</span></span><span class="line"><span class="cl">        <span class="bp">self</span><span class="o">.</span><span class="n">id2word</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__TOKENIZER</span><span class="o">.</span><span class="n">index_word</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;model/word2id.json&#34;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2id</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;word_dict length:&#34;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">word2id</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;word_dict saved in word2id.json&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">id2word</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>把所有的句子从词的层级转化为tokens</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">fit_text_to_sequences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">texts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    将所有句子转化为token
</span></span></span><span class="line"><span class="cl"><span class="s2">    :param texts:list of str,no separate in str, shape: list[str]
</span></span></span><span class="line"><span class="cl"><span class="s2">    比如:[&#34;飘渺孤鸿影，漏断人初静&#34;,”想当年，金戈铁马，气吞万里如虎“]
</span></span></span><span class="line"><span class="cl"><span class="s2">    :return:token_list:list of tokens,shape list[tokens]
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">texts</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&#34;texts must be a list[str] type&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">__char_level</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_to_wordlevel</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__TOKENIZER</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">new_texts</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">__TOKENIZER</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="数据处理词向量">数据处理（词向量）</h2>
<p>在使用词向量的情况下，分词仍然是使用jieba分词，然后使用Data_wdvec.py这个文件，文件中的函数如下：</p>
<p>创建词语字典，并返回每个词语的索引，词向量，以及每个句子所对应的词语索引</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">word2vec_train</span><span class="p">(</span><span class="n">text_sep</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">vector_size</span><span class="o">=</span><span class="n">vocab_dim</span><span class="p">,</span>  <span class="c1"># 特征向量维度</span>
</span></span><span class="line"><span class="cl">                     <span class="n">min_count</span><span class="o">=</span><span class="n">n_exposures</span><span class="p">,</span>  <span class="c1"># 可以对字典做截断. 词频少于min_count次数的单词会被丢弃掉, 默认值为5</span>
</span></span><span class="line"><span class="cl">                     <span class="n">window</span><span class="o">=</span><span class="n">window_size</span><span class="p">,</span>  <span class="c1"># 窗口大小，表示当前词与预测词在一个句子中的最大距离是多少</span>
</span></span><span class="line"><span class="cl">                     <span class="n">workers</span><span class="o">=</span><span class="n">cpu_count</span><span class="p">,</span>  <span class="c1"># 用于控制训练的并行数</span>
</span></span><span class="line"><span class="cl">                     <span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">build_vocab</span><span class="p">(</span><span class="n">text_sep</span><span class="p">)</span>  <span class="c1"># 创建词汇表， 用来将 string token 转成 index</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">text_sep</span><span class="p">,</span> <span class="n">total_examples</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">corpus_count</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;Word2vec_model.pkl&#39;</span><span class="p">)</span>  <span class="c1"># 保存训练好的模型</span>
</span></span><span class="line"><span class="cl">    <span class="n">word2idx</span><span class="p">,</span> <span class="n">word2vec</span><span class="p">,</span> <span class="n">text_sep</span> <span class="o">=</span> <span class="n">create_dictionaries</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">text_sep</span><span class="o">=</span><span class="n">text_sep</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">word2vec</span><span class="p">,</span> <span class="n">text_sep</span>  <span class="c1"># word_vectors字典类型{word:vec}</span>
</span></span></code></pre></td></tr></table>
</div>
</div><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">create_dictionaries</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">text_sep</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="p">(</span><span class="n">text_sep</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">gensim_dict</span> <span class="o">=</span> <span class="n">Dictionary</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">gensim_dict</span><span class="o">.</span><span class="n">doc2bow</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="o">.</span><span class="n">key_to_index</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">allow_update</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">w2indx</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">gensim_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>  <span class="c1"># 所有频数超过10的词语的索引</span>
</span></span><span class="line"><span class="cl">        <span class="n">w2vec</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">wv</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">w2indx</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>  <span class="c1"># 所有频数超过10的词语的词向量</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="k">def</span> <span class="nf">ToIdx</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">new_txt</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">new_txt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w2indx</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">                    <span class="k">except</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                        <span class="n">new_txt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="n">data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_txt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">return</span> <span class="n">data</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        <span class="n">text_sep</span> <span class="o">=</span> <span class="n">ToIdx</span><span class="p">(</span><span class="n">text_sep</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">text_sep</span> <span class="o">=</span> <span class="n">sequence</span><span class="o">.</span><span class="n">pad_sequences</span><span class="p">(</span><span class="n">text_sep</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">truncate_thereshold</span><span class="p">)</span>  <span class="c1"># 前方补0 为了进入LSTM的长度统一</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 每个句子所含词语对应的索引，所以句子中含有频数小于10的词语，索引为0</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">w2indx</span><span class="p">,</span> <span class="n">w2vec</span><span class="p">,</span> <span class="n">text_sep</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;No data provided...&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>构建词向量矩阵</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_data</span><span class="p">(</span><span class="n">word2idx</span><span class="p">,</span> <span class="n">word2vec</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_symbols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word2idx</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># 所有单词的索引数，频数小于10的词语索引为0，所以加1</span>
</span></span><span class="line"><span class="cl">    <span class="n">embedding_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_symbols</span><span class="p">,</span> <span class="n">vocab_dim</span><span class="p">))</span>  <span class="c1"># 索引为0的词语，词向量全为0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>  <span class="c1"># 从索引为1的词语开始，对每个词语对应其词向量</span>
</span></span><span class="line"><span class="cl">        <span class="n">embedding_weights</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">word2vec</span><span class="p">[</span><span class="n">word</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">n_symbols</span><span class="p">,</span> <span class="n">embedding_weights</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="搭建模型">搭建模型</h2>
<ul>
<li>Embedding层
<ul>
<li>词嵌入，将index转化为vector</li>
</ul>
</li>
<li>Bi-LSTM层</li>
<li>Dense层
<ul>
<li>通过softmax激活，映射到多分类问题</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">define_lstm_model</span><span class="p">(</span><span class="n">word2token_len</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input_shape是输入的维度特征,output_dim是输出词向量的维度，是一个随机初始化的过程</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span><span class="n">input_dim</span><span class="o">=</span><span class="n">word2token_len</span><span class="p">,</span><span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">LSTM_UNITS</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&#34;softmax&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;accuracy&#34;</span><span class="p">],</span><span class="n">loss</span><span class="o">=</span><span class="s2">&#34;sparse_categorical_crossentropy&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>以及</p>
<ul>
<li>
<p>Embedding层</p>
<ul>
<li>词嵌入，将index转化为vector</li>
</ul>
</li>
<li>
<p>Conv1D层</p>
<ul>
<li>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div></li>
</ul>
</li>
<li>
<p>BiLSTM层</p>
</li>
<li>
<p>Dense层</p>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">define_word_avg_model</span><span class="p">(</span><span class="n">word2token</span><span class="p">,</span><span class="n">truncate_thereshold</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># input_shape是输入的维度特征,output_dim是输出词向量的维度，是一个随机初始化的过程</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span><span class="n">input_dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">word2token</span><span class="p">),</span><span class="n">input_length</span><span class="o">=</span><span class="n">truncate_thereshold</span><span class="p">,</span><span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 这里需要些模型</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># filters是输出的向量长</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv1D</span><span class="p">(</span><span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span><span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">LSTM_UNITS</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s2">&#34;softmax&#34;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&#34;accuracy&#34;</span><span class="p">],</span><span class="n">loss</span><span class="o">=</span><span class="s2">&#34;sparse_categorical_crossentropy&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">model</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>若使用词向量，则使用单向LSTM，嵌入层使用词向量矩阵初始化</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">define_lstm_model</span><span class="p">(</span><span class="n">n_symbols</span><span class="p">,</span> <span class="n">embedding_weights</span><span class="p">,</span> <span class="n">truncate_thereshold</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Defining a Simple Keras Model...&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>  <span class="c1"># or Graph or whatever #堆叠</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 嵌入层将正整数（下标）转换为具有固定大小的向量</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">,</span>  <span class="c1"># 词向量的维度</span>
</span></span><span class="line"><span class="cl">                        <span class="n">input_dim</span><span class="o">=</span><span class="n">n_symbols</span><span class="p">,</span>  <span class="c1"># 字典(词汇表)长度</span>
</span></span><span class="line"><span class="cl">                        <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  <span class="c1"># 确定是否将输入中的‘0’看作是应该被忽略的‘填充’（padding）值</span>
</span></span><span class="line"><span class="cl">                        <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">embedding_weights</span><span class="p">],</span> <span class="c1"># 词向量矩阵</span>
</span></span><span class="line"><span class="cl">                        <span class="n">input_length</span><span class="o">=</span><span class="n">truncate_thereshold</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>  <span class="c1"># 全连接层</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Compiling the Model...&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                  <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="训练模型">训练模型</h2>
<p>以word_lstm为例，首先定义checkpoint，以验证集的损失为单位，保存</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 定义一个CheckPoint</span>
</span></span><span class="line"><span class="cl"><span class="n">CheckPoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&#34;val_loss&#34;</span><span class="p">,</span><span class="n">filepath</span><span class="o">=</span><span class="s2">&#34;model/word_lstm&#34;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>然后训练，并且保存历史，以供后续制图</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">pad_seqs</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="n">EPOCHES</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">CheckPoint</span><span class="p">],</span><span class="n">class_weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">,</span><span class="n">validation_split</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s2">&#34;model/word_lstm_model.h5&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&#34;model/word_lstm_history.txt&#34;</span><span class="p">,</span><span class="n">mode</span><span class="o">=</span><span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">,</span><span class="n">f</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="超参数说明">超参数说明</h2>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">EPOCHES</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl"><span class="n">output_dim</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl"><span class="n">LSTM_UNITS</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl"><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>
</span></span><span class="line"><span class="cl"><span class="n">filters</span> <span class="o">=</span> <span class="mi">30</span>
</span></span><span class="line"><span class="cl"><span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="n">strides</span> <span class="o">=</span> <span class="mi">1</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="实验结果和说明">实验结果和说明</h2>
<h2 id="总结">总结</h2>
<p>一开始做实验的时候，是因为以字符形式输入到嵌入层，和中文语义中的分词是有所区别，而且违反直觉的。而且在英文中，却是分英文单词，而不是分英文字符的。在课程设计中，我们决定做一个实验去探索这一点。由于词相比字的稀疏性，在本数据集40000条左右的情况下，可以看到，中文分词输入由于词比字更稀疏，过拟合非常的明显，以非常少的词就可以判断训练集的分类。但这些词对验证集来说，可能一点影响没有，或者验证集中，干脆找不到这些词，这些词可能出现的频率就很低。</p>
<p>在不断地学习过程中，词输入的BiLSTM层，更进一步的，学习了无关紧要的信息，进一步精细化的减少训练集的损失，这也使得学习到的词越来越对分类有决定性作用，可能就是生僻词越来越多，这也使得测试集的loss在不断地增加，acc出现了下降后的震荡。</p>
<p>一个可能的解决方法，是先对训练集的词做一个筛选，只留下高频词，这会提升以词为输入的训练模型的准确度。</p>
<p>以词为输入的训练模型，在训练集上的表现比字输入要更好，但是过拟合的情况也特别明显，需要做额外的步骤处理，泛化。</p>
<p>对于大部分情况来说，以字为输入虽然收敛要慢一些，但是过拟合的情况，即使发生，也可以通过正则化或者Dropout等手段调节。而对词来说，这种严重的过拟合趋势，必须要对输入做一些处理。</p>
</div></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.105.0">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">xxxx</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/contrib/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
